{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.utils.data import random_split\nfrom torchvision import models,datasets\nimport os\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom PIL import Image\nfrom torch.optim import lr_scheduler\nimport cv2\nimport albumentations as A\nimport pandas as pd\nfrom albumentations.pytorch import ToTensorV2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\n    device = 'cuda'\nelse:\n    print('using device: cpu')\n\ndevice = torch.device(\"cuda:0\" if USE_GPU else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters etc.\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 2e-5\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 32\nTEST_BATCH_SIZE = 128\nNUM_EPOCHS = 50\nIMAGE_HEIGHT = 160\nIMAGE_WIDTH = 240\nPIN_MEMORY = True\nLOAD_MODEL = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35, p=1.0),\n        A.HorizontalFlip(p=.1),\n        A.VerticalFlip(p=.5),\n        A.Normalize(\n            mean=[0., 0., 0.],\n            std=[1., 1., 1.],\n            max_pixel_value=255.,\n        ),\n        ToTensorV2(),\n    ]\n)\n\nval_transforms = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean=[0., 0., 0.],\n            std=[1., 1., 1.],\n            max_pixel_value=255.,\n        ),\n        ToTensorV2(),\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_transform = A.Compose([A.Resize(256, 256, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(), A.VerticalFlip(), \n                     A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n                     A.GaussNoise()])\n\n\nmask_transform = A.Compose([A.Resize(256, 256, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(), A.VerticalFlip(), \n                     A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n                     A.GaussNoise()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\n\nwith zipfile.ZipFile(\"../input/carvana-image-masking-challenge/train.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\".\")\n\nwith zipfile.ZipFile(\"../input/carvana-image-masking-challenge/train_masks.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv_zip = '../input/carvana-image-masking-challenge/train_masks.csv.zip'\n\nwith zipfile.ZipFile(train_csv_zip, 'r') as zip_:\n    zip_.extractall(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's make a dataframe that maps the car_id to the image_path\nimg_id_list = []\npath_list = []\nfor dir_name, _, file_names in os.walk('./train'):\n    for file_name in file_names:\n        path = os.path.join(dir_name, file_name)\n        path_list.append(path)\n        img_id = file_name.split('.')[0]\n        img_id_list.append(img_id)\n\ndictionary = {'id':img_id_list, 'img_path':path_list}\ndf = pd.DataFrame(data = dictionary)\ndf = df.set_index('id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's make another dataframe that maps the car_ids to the mask_path\nimg_id_list = []\npath_list = []\nfor dir_name, _, file_names in os.walk('./train_masks'):\n    for file_name in file_names:\n        path = os.path.join(dir_name, file_name)\n        path_list.append(path)\n        img_id = file_name.split('_mask')[0]\n        img_id_list.append(img_id)\n\ndictionary = {'id':img_id_list, 'mask_path':path_list}\nmask_df = pd.DataFrame(data = dictionary)\nmask_df = mask_df.set_index('id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now, let's add the mask path's to the same dataframe whci halready has the car_ids and image_paths\ndf['mask_path'] = mask_df['mask_path']\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class U_NetDataset(Dataset):\n    def __init__(self,df,transform = None):\n        self.df = df \n        self.transform = transform\n        self.images = df['img_path'].tolist()\n        self.masks = df['mask_path'].tolist()\n        assert len(self.images) == len(self.masks), 'number of images and masks are not equal'\n        \n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self,idx):\n        image_path = self.images[idx]\n        mask_path = self.masks[idx]\n        #The Albumentations library requires images and masks to be a numpy array\n        img = np.array(Image.open(image_path).convert('RGB'))\n        mask = np.array(Image.open(mask_path).convert('L'),dtype = np.float32) #Convert to grayscale\n        mask[mask == 255.0] = 1.0 #Since our loss function is going to have a sigmoid, we'll convert our mask into 1s and 0s\n        #The Albumentations library requires this format\n        augmentations = self.transform(image = img, mask = mask)\n        image = augmentations[\"image\"]\n        mask = augmentations[\"mask\"]\n                \n        return image,mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(df, test_size=.1)\n\ntrain_dataset = U_NetDataset(train_df, train_transforms)\nprint(len(train_dataset))\nval_dataset = U_NetDataset(val_df, val_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=PIN_MEMORY)\nval_loader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, pin_memory=PIN_MEMORY)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity Check\ndataiter = iter(train_loader)\nimages, masks = dataiter.next()\n\nprint(images.shape)\nprint(masks.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy\nsamples, labels = iter(train_loader).next()\nplt.figure(figsize=(16,32))\ngrid_imgs = torchvision.utils.make_grid(samples[:32])\nnp_grid_imgs = grid_imgs.numpy()\n# in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\nplt.imshow(numpy.transpose(np_grid_imgs, (1,2,0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rand_idx = np.random.randint(len(train_dataset), size=5)\n\nfig, axes = plt.subplots(5, 2, figsize=(15, 20))\n\n# Plot the images\nfor i, idx in enumerate(rand_idx):\n    img, mask = train_dataset[idx]\n    img = img.numpy()\n    mask = mask.numpy()\n    # in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\n    img = img.transpose(1, 2, 0)\n    \n    ax_img = axes[i][0]\n    ax_mask = axes[i][1]\n    \n    ax_img.imshow(img)\n    ax_img.get_xaxis().set_visible(False)\n    ax_img.get_yaxis().set_visible(False)\n    ax_img.set_title('original')\n    \n    ax_mask.imshow(mask)\n    ax_mask.get_xaxis().set_visible(False)\n    ax_mask.get_yaxis().set_visible(False)\n    ax_mask.set_title('mask')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model\n#We are going to make a couple ofsmall changes in the implementation, compared to the original unet paper, we are going to use padded cpnvolution and not downsize and we'll use BatchNorm.\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False), #Same padding, bias is False because we are using BatchNorm\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace = True),\n        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace = True)\n        )\n        \n    def forward(self, x):\n        return self.conv(x)\n    \nclass UNET(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features = [64,128,256,512]):\n        super(UNET, self).__init__()\n        #Define two lists to store all the Conv layers and also define a pooling layer\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList() \n        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n        #Define downsampling\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels,feature)),\n            in_channels = feature\n                \n        #Define Upsampling\n        for feature in reversed(features):\n            #Set kernel_size and stride to double the image_height and image_width\n            self.ups.append(\n            nn.ConvTranspose2d(feature*2,feature,kernel_size = 2,stride = 2)\n            )\n            self.ups.append(DoubleConv(feature*2,feature))\n        #This is the layer which is at the bottom of the U shape\n        self.bottleneck = DoubleConv(features[-1],features[-1]*2)\n        #Set kernel_size to maintain the height and width of the image\n        self.final_conv = nn.Conv2d(features[0],out_channels,kernel_size = 1)\n        \n    def forward(self,x):\n        skip_connections = []\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x) #To remember the Conv we need in the skip connections\n            x = self.pool(x)\n            \n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n            \n        for idx in range(0,len(self.ups),2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n            #But what if input image dimensions are not divisible by 2^^4?\n            if x.shape != skip_connection.shape:\n                x = F.resize(x, size=skip_connection.shape[2:])#The [2:] gets rid of the batch size and number of dim\n            \n            concat_skip = torch.cat((skip_connection, x),dim = 1)\n            x = self.ups[idx+1](concat_skip)\n            \n        return self.final_conv(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary\nm = UNET(in_channels= 3, out_channels = 1).to(device)\nsummary(m, (3,256,256))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNET(in_channels = 3,out_channels = 1).to(device)\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ndef train(loader, model, optimizer, criterion, scaler):\n    loop = tqdm(loader) #For the progress bar\n    \n    for idx, (data, targets) in enumerate(loop):\n        data = data.to(device)\n        targets = targets.float().unsqueeze(1).to(device)\n        #forward pass\n        with torch.cuda.amp.autocast():\n            pred = model(data)\n            loss = criterion(pred, targets)\n        #backward pass\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        loop.set_postfix(loss = loss.item())\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_score(loader, model, device = \"cuda\"):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n    with torch.no_grad():\n        for x,y in loader:\n            x=x.to(device)\n            y=y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds>0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels +=torch.numel(preds)\n            dice_score += (2*(preds*y).sum())/((preds+y).sum()+1e-8)\n        print(f\"Got{num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")  \n        print(f\"Dice score : {dice_score/len(loader)}\")\n        model.train()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscaler = torch.cuda.amp.GradScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 10\nfor epoch in range(NUM_EPOCHS):\n    print(f\"epoch:{epoch}/{NUM_EPOCHS}\")\n    train(train_loader, model, optimizer, criterion, scaler)\n    dice_score(val_loader, model, device = \"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_save_name = 'UNET_Carvana.pt'\npath = F\".//{model_save_name}\" \ntorch.save(model.state_dict(), path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Test(Dataset):\n    def __init__(self,df,transform = None):\n        self.df = df \n        self.transform = transform\n        self.images = df['img_path'].tolist()\n        #self.masks = df['mask_path'].tolist()\n        #assert len(self.images) == len(self.masks), 'number of images and masks are not equal'\n        \n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self,idx):\n        image_path = self.images[idx]\n        #mask_path = self.masks[idx]\n        #The Albumentations library requires images and masks to be a numpy array\n        img = np.array(Image.open(image_path).convert('RGB'))\n        #mask = np.array(Image.open(mask_path).convert('L'),dtype = np.float32) #Convert to grayscale\n        #mask[mask == 255.0] = 1.0 #Since our loss function is going to have a sigmoid, we'll convert our mask into 1s and 0s\n        #The Albumentations library requires this format\n        augmentations = self.transform(image = img)\n        image = augmentations[\"image\"]\n        #mask = augmentations[\"mask\"]\n                \n        return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rand_idx = np.random.randint(len(train_dataset), size=5)\n\nfig, axes = plt.subplots(BATCH_SIZE, 2, figsize=(15, 80))\n\n# Plot the images\nfor i,(img, mask) in enumerate(train_loader):\n    img = img.to(device)\n    mask1 = model(img)\n    img = img.cpu().numpy()\n    mask1 = mask1.cpu().detach().numpy()\n    # in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\n    img = img.transpose(0,2, 3, 1)\n    mask1 = mask1.transpose(0,2, 3, 1)\n    ax_img = axes[i][0]\n    ax_mask = axes[i][1]\n    \n    ax_img.imshow(img[i])\n    ax_img.get_xaxis().set_visible(True)\n    ax_img.get_yaxis().set_visible(True)\n    ax_img.set_title('original')\n    \n    ax_mask.imshow(mask1[i])\n    ax_mask.get_xaxis().set_visible(True)\n    ax_mask.get_yaxis().set_visible(True)\n    ax_mask.set_title('mask')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with zipfile.ZipFile(\"../input/carvana-image-masking-challenge/test.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_id_list = []\npath_list = []\nfor dir_name, _, file_names in os.walk('./test'):\n    for file_name in file_names:\n        path = os.path.join(dir_name, file_name)\n        path_list.append(path)\n        img_id = file_name.split('.')[0]\n        img_id_list.append(img_id)\n\ntdictionary = {'id':img_id_list, 'img_path':path_list}\ndft = pd.DataFrame(data = tdictionary)\ndft = dft.set_index('id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Test(dft, val_transforms)\ntest_loader = DataLoader(test_ds, batch_size=32, shuffle=False, pin_memory=PIN_MEMORY)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in test_loader:\n    i = i.to(device)\n    pred = model(i)\n    #dice_score(test_loader, model, device = \"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" rand_idx = np.random.randint(len(test_ds), size=5)\n\nfig, axes = plt.subplots(BATCH_SIZE-1, 2, figsize=(15, 80))\n\n# Plot the images\nfor i,(img) in enumerate(test_loader):\n    img = img.to(device)\n    mask1 = model(img)\n    img = img.cpu().numpy()\n    mask1 = mask1.cpu().detach().numpy()\n    # in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\n    img = img.transpose(0,2, 3, 1)\n    mask1 = mask1.transpose(0,2, 3, 1)\n    ax_img = axes[i][0]\n    ax_mask = axes[i][1]\n    \n    ax_img.imshow(img[i])\n    ax_img.get_xaxis().set_visible(True)\n    ax_img.get_yaxis().set_visible(True)\n    ax_img.set_title('original')\n    \n    ax_mask.imshow(mask1[i])\n    ax_mask.get_xaxis().set_visible(True)\n    ax_mask.get_yaxis().set_visible(True)\n    ax_mask.set_title('mask')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}